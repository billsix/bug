%Copyright 2014-2016 - William Emerison Six
%All rights reserved
%Distributed under LGPL 2.1 or Apache 2.0

\appendix
 \appendixpage
 \noappendicestocpagenum
 \chapter{Compile-Time Language}
  \label{sec:appendix1}
 This appendix provides a quick tour of computer language which is interpreted
 by the compiler, but which is absent in the generated machine
 code.  Examples are provided in well-known languages to illustrate that
 many compilers are also interpreters for a subset of the language.  This
 appendex provides a baseline understanding of compile-time computation
 so that the reader may contrast these languages' capabilities with libbug's.
 But first, let's discuss was is meant by the words ``language'',``compiler'', and
 ``interpreter''.

 In ``Introduction to Automata Theory, Languages, and Computation'', Hopcroft,
 Motwani, and Ullman define language as ``A set of strings all of which are chosen
 from some $\Sigma^{\star}$, where $\Sigma$ is a particular alphabet, is called
 a language'' \cite[p. 30]{hmu2001}.
 % They further state ``In automata theory, a problem is the question
 % of deciding whether a given string is a member of some particular language''. \cite[p. 31]{hmu2001}.
 Plainly, that means an ``alphabet'' is a set of characters (for instance, ASCII), and
 that a computer ``language'' is defined as all of the possible sequences of characters
 from that alphabet which match some criterion.

 An ``interpreter'' is a pre-existing computer program which takes a specific
 computer language as input,
 and does some action based on it.  A ``compiler'' is a pre-existing computer program
 which takes computer language as input,
 but rather than immediately doing the actions specified by the input, instead the compiler
 translates the input language
 into another computer language (typically machine code), which is then output to a file
 for interpretation\footnote{the CPU can be viewed as an
 interpreter which takes machine code as its' input} at a later time.

 In practice though, the distinction is not clear cut.  Most compilers do not exclusively
 translate from an input language
 to an output language; instead, they also interpret a subset of the input
 language as part of the process of generating the output language.  Well, what
 types of computations can be performed by this subset of language, and how do
 they vary in expressive power?
;
 \section{C}
Consider the following C code:

 \begin{examplecode}
/*Line01*/  #include <stdio.h>
/*Line02*/  #define square(x) ((x) * (x))
/*Line03*/  int fact(unsigned int n);
/*Line04*/  int main(int argc, char* argv[]){
/*Line05*/  #ifdef DEBUG
/*Line06*/    printf("Debug - argc = %d\n", argc);
/*Line07*/  #endif
/*Line08*/   printf("%d\n",square(fact(argc)));
/*Line09*/    return 0;
/*Line10*/  }
/*Line11*/  int fact(unsigned int n){
/*Line12*/    return n == 0
/*Line13*/      ? 1
/*Line14*/      : n * fact(n-1);
/*Line15*/  }
 \end{examplecode}

 \begin{itemize}
  \item
     On the first line, the \#include preprocessor command
     is language that the compiler
     is intended to interpret, instructing the compiler to
     read the file ``stdio.h''
     from the filesystem and to splice the content
     into the current C file.  The include command
     itself has no representation in the generated machine code, although the contents
     of the included file may.

  \item
     The second line defines a C macro. C macros are procedure definitions which
      are not to be translated into the output language, but instead it is a procedure
     to be interpreted by the compiler
     at compile-time only.  C macros take a text
     string as input and transforms it into a new text string as output.
     This expansion happens before the compiler does much
     else.  For example, using GCC as a compiler, if you run the C preprocessor
     ``cpp'' on the above C code, you'll see that

     \begin{examplecode}
  printf("%d\n",square(fact(argc)));
     \end{examplecode}

     \noindent expands into

     \begin{examplecode}
  printf("%d\n",((fact(argc)) * (fact(argc))));
     \end{examplecode}

     \noindent before compilation.

  \item
     The third line defines a procedure prototype, so that
     the compiler knows the argument types and return type for a procedure not
     yet defined called ``fact''.
     It is language interpreted by the compiler to determine the types for the procedure
     call to ``fact'' on line 8, since ``fact'' has not yet been defined in this
     translation unit.
  \item
     The fourth through tenth line is a procedure definition, which will be
     translated into instructions in the generated machine code.  Line 5, however, is language
     to be interpreted by the compiler, referencing a variable which is defined
     only during compilation, to detemine whether or not line 6 should be
     compiled.
 \end{itemize}

 \section{C++}

 C++ inherits C's macros, but with the additional introduction
 of templates, C++'s compile-time language
 incidentally became Turing complete.  This means that
 anything that can be
 calculated by a computer can be calculated using template expansion
 at compile-time.  That's great!  So how does a programmer begin to use this new
 expressive power?

 The following is an example of calculating the factorial of
 3, using C++ procedures for run-time calulation, and C++'s templates for compile-time
 calculation.

 \begin{examplecode}
 /*Line01*/  #include <iostream>
 /*Line02*/  template <unsigned int n>
 /*Line03*/  struct factorial {
 /*Line04*/      enum { value = n * factorial<n - 1>::value };
 /*Line05*/  };
 /*Line06*/  template <>
 /*Line07*/  struct factorial<0> {
 /*Line08*/      enum { value = 1 };
 /*Line09*/  };
 /*Line10*/  int fact(unsigned int n){
 /*Line11*/    return n == 0
 /*Line12*/      ? 1
 /*Line13*/      : n * fact(n-1);
 /*Line14*/  }
 /*Line15*/  int main(int argc, char* argv[]){
 /*Line16*/    std::cout << factorial<3>::value << std::endl;
 /*Line17*/    std::cout << fact(3) << std::endl;
 /*Line18*/    return 0;
 /*Line19*/  }
 \end{examplecode}

 \begin{itemize}
  \item
    Lines 10-13 are the run-time calculation of ``fact'', identical
    to the previous version in C.
  \item
   Lines 2-9 are the
   template code for the compile-time calculation of ``factorial''.
   \item
 On line 16, ``factorial\textless3\textgreater::value'' is an
 language to be interpreted
 by the compiler via template expansions.  Template expansions
 conditionally match patterns based on types (or values in the case
 of integers).  For iteration, instead of loops, templates expand recursively.
 In this case, the expansion of
 ``factorial\textless3\textgreater::value'' in dependent upon
 ``factorial\textless n-1\textgreater::value''.  The compiler
 does the subtraction during compile-time,
 so ``factorial\textless3\textgreater::value'' is dependent on
 ``factorial\textless2\textgreater::value''.
 This recursion will terminate on ``factorial\textless0\textgreater::value''
 on line 7. (Even though
 the base case of ``factorial\textless0\textgreater'' is specified
 after the more general
 case of ``factorial\textless n\textgreater'', template expansion expands to the most
 specific case first.  So the compiler will terminate.)

   \item
 On line 17, a run-time call to ``fact'', defined on line 10, is declared.
 \end{itemize}

 The drastic difference in the generated code can be observed by using ``objdump -D''.

 \begin{examplecode}
 400850: be 06 00 00 00   mov    $0x6,%esi
 400855: bf c0 0d 60 00   mov    $0x600dc0,%edi
 40085a: e8 41 fe ff ff   callq  4006a0 <_ZNSolsEi@plt>
 .......
 .......
 .......
 40086c: bf 03 00 00 00   mov    $0x3,%edi
 400871: e8 a0 ff ff ff   callq  400816 <_Z4facti>
 400876: 89 c6            mov    %eax,%esi
 400878: bf c0 0d 60 00   mov    $0x600dc0,%edi
 40087d: e8 1e fe ff ff   callq  4006a0 <_ZNSolsEi@plt>
 \end{examplecode}

 \begin{itemize}
   \item
 The instructions at memory locations 400850 through 40085a correspond to the
 printing of the compile-time expanded call to factorial\textless3\textgreater.
 The number 6 is loaded into the esi register; then the second
 two lines call the printing routine\footnote{at least I assume, because
 I don't completely understand how C++ name-mangling works}.
   \item
 The instructions at locations 40086c through 40087d correspond to the
 printing of the run-time calculation to fact(3).  The number 3
 is loaded into the edi register, fact is invoked, the result of
 calling fact is moved from the eax register to the esi register, and then
 printing routine is called.

 \end{itemize}
 The compile-time computation worked!

 \section{libbug}
 Like C++, Libbug's compile-time language is Turing complete.
 With libbug, the following is how factorial is defined, and how
 the factorial of 3 is printing to the screen, both computed at
 compile-time and computed at run-time.

 \begin{examplecode}
 {at-both-times                           ;; Line 1
  {define fact                            ;; Line 2
    [|n| (if (= n 0)                      ;; Line 3
             [1]                          ;; Line 4
             [(* n (fact (- n 1)))])]}}   ;; Line 5
                                          ;; Line 6
 (pp (at-compile-time-expand (fact 3)))   ;; Line 7
 (pp (fact 3))                            ;; Line 8
 \end{examplecode}

 \begin{itemize}
   \item
      On line 1, the ``at-both-times'' macro is invoked, taking the unevaluated
      definition of ``fact'' as
      as argument, interpreting it at compile-time, and compiling it for use at runtime.
   \item
      On lines 2-5, the definition of the ``fact''.
   \item
      On line 7, ``at-compile-time-expand'' is a macro which takes unevaluated code,
      evaluates it to some result which is then compiled by the compiler.  So the code
      will expand at compile-time to ``(pp 6)''.
   \item
      On line 8, the run-time calculation of ``(fact 3)''.
 \end{itemize}

 By compiling the Scheme source to the ``gvm'' intermediate
 representation, the stated behavior can be verified.

 \begin{examplecode}
  ...
  r1 = '6                                ;; Line 1
  r0 = #4                                ;; Line 2
  jump/safe fs=4 global[pp] nargs=1      ;; Line 3
#4 fs=4 return-point                     ;; Line 4
  r1 = '3                                ;; Line 5
  r0 = #5                                ;; Line 6
  jump/safe fs=4 global[fact] nargs=1    ;; Line 7
#5 fs=4 return-point                     ;; Line 8
  r0 = frame[1]                          ;; Line 9
  jump/poll fs=4 #6                      ;; Line 10
#6 fs=4                                  ;; Line 11
  jump/safe fs=0 global[pp] nargs=1      ;; Line 12
  ...
 \end{examplecode}

 \begin{itemize}
   \item
      Lines 1-4 correspond to ``(pp (at-compile-time-expand (fact 3)))''.  The precomputed
      value of ``(fact 3)'' is 6, which is directly stored into a GVM register, and
      then the ``pp'' routine is called to print it out.
   \item
      Lines 5-12 correspond to ``(pp (fact 3))''.  3 is stored in a GVM regiister, ``fact''
      is called, and then ``pp'' is called on the result.
 \end{itemize}

 \section{Comparison of Power}

 Although C++'s and libbug's compile-time languages are both Turing complete,
 they vary drastically in actual real-world programming power.  The language used
 for compile-time calculation of ``fact'' in C++ is a drastically different language than
 the one used for run-time.  Although not demonstrated in this book,
 C++ template metaprogramming relies exclusively on recursion for repetition (it has no
 looping construct), it has no mutable state, and it lacks the ability to do input/output
 (I/O)\footnote{For the masochist who wants to know more about C++'s compile-time language,
 I recommend \cite{ctm} }

 In contrast, the compile-time
 language in libbug is the same exact language as the one that the compiler
 is compiling, complete with state and I/O!  How can that power be used?
 This book is the beginning of an answer.


 When I first started creating libbug, I only wanted to collocate
 tests with definitions, to evaluate the tests at compile-time exclusively, and to error out
 of compilation
 if any test failed.  Only later did I discover that compile-time evaluation
 can execute full programs without limitations.  What else could be
 calculated at compile-time?  In graphics, perhaps calculating normal vectors
 from a vertex mesh automatically; perhaps writing ``shaders'' as compile-time
 tested Scheme procedures which are translated into actual shaders at compile-time.
 In database programming, perhaps fetching the table definitions at compile-time,
 and generating code for easy database access.


 \chapter{Why Lisp Needs Lambda Literals}
  \label{sec:appendixliteral}

 Foo bar.


 \bibliography{abbr_long,pubext}
\begin{thebibliography}{9}

\bibitem[Abelson96]{sicp}
  Abelon, Harold, Gerald Jay Sussman, and Julie Sussman.
  \emph{Structure and Interpretation of Computer Programs},
  The MIT Press, Massachusetts,
  Second Edition,
  1996.
\bibitem[Abrahams2004]{ctm}
  Abrahams, David and Aleksey Gurtovoy
  \emph{C++ Template Metaprogramming},
  Addison Wesley
  2004.

\bibitem[Church51]{calculi}
  Church, Alonzo
  \emph{The Calculi of Lambda-Conversion},
  Princeton University Press, New Jersey,
  Second Printing,
  1951.

\bibitem[Dybvig03]{schemeprogramminglanguage}
  Dybvig, R. Kent.
  \emph{The Scheme Programming Language},
  The MIT Press, Massachusetts,
  Third Edition,
  2003.

\bibitem[Feeley12]{evalduringmacroexpansion}
  Feeley, Marc. https://mercure.iro.umontreal.ca/pipermail/gambit-list/2012-April/005917.html, 2012

\bibitem[Friedman96]{littleschemer}
  Friedman, Daniel P., and Matthias Felleisen
  \emph{The Scheme Programming Language},
  The MIT Press, Massachusetts,
  Fourth Edition,
  1996.
\bibitem[Graham94]{onlisp}
  Graham, Paul.
  \emph{On Lisp},
  Prentice Hall, New Jersey,
  1994.

\bibitem[Graham96]{ansicl}
  Graham, Paul.
  \emph{ANSI Common Lisp},
  Prentice Hall, New Jersey,
  1996.

\bibitem[Harvey01]{ss}
  Harvey, Brian and Matthew Wright.
  \emph{Simply Scheme - Introducing Computer Science},
  The MIT Press, Massachusetts,
  Second Edition,
  2001.

\bibitem[Hopcroft01]{hmu2001}
  Hopcroft, John E., Rajeev Motwani, and Jeffrey D. Ullman.
  \emph{Introduction to Automata Theory, Languages, and Computation},
  Addison Wesley, Massachusetts,
  Second Edition,
  2001.

\bibitem[Kiselyov98]{setf}
  Kiselyov, Oleg. http://okmij.org/ftp/Scheme/setf.txt , 1998.
\bibitem[Knuth97]{taocp}
  Knuth, Donald E.
  \emph{The Art Of Computer Programming, Volume 1},
  Addison Wesley, Massachusetts,
  Third Edition,
  1997.
\bibitem[Norvig92]{paip}
  Norvig, Peter
  \emph{Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp},
  San Francisco, CA
  1992.
\bibitem[Pierce02]{tapl}
  Pierce, Benjamin C.
  \emph{Types and Programming Languages},
  The MIT Press
  Cambridge, Massachusetts
  2002.
\bibitem[Stallings03]{crypto}
  Stallings, William
  \emph{Cryptography and Network Security},
  Pearson Education, Upper Saddle River, New Jersey,
  Third Edition,
  2002.
\bibitem[Steele90]{cl}
  Steele Jr, Guy L.
  \emph{Common Lisp the Language},
  Digital Press,
  1990.




\end{thebibliography}
 \printindex

\end{document}  %End of document.
