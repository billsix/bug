%Copyright 2014-2017 - William Emerison Six
%All rights reserved
%Distributed under LGPL 2.1 or Apache 2.0

\appendix
 \appendixpage
 \noappendicestocpagenum
 \chapter{Compile-Time Language}
  \label{sec:appendix1}
 This appendix\footnote{Examples in the appendix will have boxes
 and line numbers around the code, but they are not part of libbug.}
 provides a quick tour of computer language which is interpreted
 by the compiler but which is absent in the generated machine
 code.  Examples are provided in
 well-known languages to illustrate that
 most compilers are also interpreters for a subset of the language.  This
 appendix provides a baseline demonstration of compile-time computation
 so that the reader may contrast these languages' capabilities with libbug's.
 But first, let's discuss what is meant by the words ``language'',``compiler'', and
 ``interpreter''.

 In ``Introduction to Automata Theory, Languages, and Computation'', Hopcroft,
 Motwani, and Ullman define language as ``A set of strings all of which are chosen
 from some $\Sigma^{\star}$, where $\Sigma$ is a particular alphabet, is called
 a language'' \cite[p. 30]{hmu2001}.
 % They further state ``In automata theory, a
 % problem is the question
 % of deciding whether a given string is a member of some particular
 % language''. \cite[p. 31]{hmu2001}.
 Plainly, that means that an ``alphabet'' is a set of characters (for instance, ASCII), and
 that a computer ``language'' is defined as all of the possible sequences of characters
 from that alphabet which are able to be compiled successfully.

 An ``interpreter'' is a computer program which takes an instance of a specific
 computer language as input,
 and immediately executes the instructions.  A ``compiler'' is a computer program
 which also takes an instance of a specific computer language as input,
 but rather than immediately executing the input language, instead the compiler
 translates the input language
 into another computer language (typically machine code), which is then output to a file
 for interpretation\footnote{the Central Processing Unit (CPU) can be viewed as an
 interpreter which takes machine code as its input} at a later time.

 In practice though, the distinction is not binary.  Most compilers do not exclusively
 translate from an input language
 to an output language; instead, they also interpret a subset of the input
 language as part of the compilation process.  So what
 types of computations can be performed by this subset of language, and how do
 they vary in expressive power?

 \section{C}
Consider the following C code:

 \begin{code}
#include <stdio.h>
#define square(x) ((x) * (x))
int fact(unsigned int n);
int main(int argc, char* argv[]){
#ifdef DEBUG
  printf("Debug - argc = %d\n", argc);
#endif
 printf("%d\n",square(fact(argc)));
  return 0;
}
int fact(unsigned int n){
  return n == 0
    ? 1
    : n * fact(n-1);
}
 \end{code}

 \begin{itemize}
  \item
     On line 1, the \#include preprocessor command
     is language to be interpreted by the compiler,
     instructing the compiler to
     read the file ``stdio.h''
     from the filesystem and to splice the content
     into the current C file.  The \#include command
     itself has no representation in the generated machine code, although the contents
     of the included file may.

  \item
     Line 2 defines a C macro. A C macro is a procedure definition which
     is to be interpreted by the compiler instead of being translated
     into the output language.
     A C macro takes a text
     string as input and transforms it into a new text string as output.
     This expansion happens before the compiler does anything
     else.  For example, using GCC as a compiler, if you run the C preprocessor
     ``cpp'' on the above C code, you'll see that

     \begin{code}
  printf("%d\n",square(fact(argc)));
     \end{code}

     \noindent expands into

     \begin{code}
  printf("%d\n",((fact(argc)) * (fact(argc))));
     \end{code}

     \noindent before compilation.

  \item
     Line 3 defines a procedure prototype so that
     the compiler knows the argument types and the return type for a procedure not
     yet defined called ``fact''.
     It is language interpreted by the compiler to determine the types for the procedure
     call to ``fact'' on line 8.
  \item
     Lines 4 through 10 are a procedure definition which will be
     translated into instructions in the generated machine code.  Line 5 however, is language
     to be interpreted by the compiler, referencing a variable which is defined
     only during compile-time, to detemine whether or not line 6 should be
     compiled.
 \end{itemize}

 \section{C++}

 C++ inherits C's macros, but with the additional introduction
 of templates, its compile-time language
 incidentally became Turing complete;  meaning that
 anything that can be
 calculated by a computer can be calculated using template expansion
 at compile-time.  That's great!  So how does a programmer use this new
 expressive power?

 The following is an example of calculating the factorial of
 3; using C++ procedures for run-time calculation, and C++'s templates for compile-time
 calculation.

 \begin{code}
 #include <iostream>
 template <unsigned int n>
 struct factorial {
     enum { value = n * factorial<n - 1>::value };
 };
 template <>
 struct factorial<0> {
     enum { value = 1 };
 };
 int fact(unsigned int n){
   return n == 0
     ? 1
     : n * fact(n-1);
 }
 int main(int argc, char* argv[]){
   std::cout << factorial<3>::value << std::endl;
   std::cout << fact(3) << std::endl;
   return 0;
 }
 \end{code}

 \begin{itemize}
  \item
    Lines 10-14 are the run-time calculation of ``fact'', identical
    to the previous version in C.
  \item
   Lines 2-9 are the
   template code for the compile-time calculation of ``factorial''.  Notice
    that the language constructs used are drastically different than the
    run-time constructs.
   \item
 On line 16, ``factorial\textless3\textgreater::value'' is
 language to be interpreted
 by the compiler via template expansions.  Template expansions
 conditionally match patterns based on types (or values in the case
 of integers).  For iteration, templates expand recursively instead of using loops.
 In this case,  ``factorial\textless3\textgreater::value'' expands to
 ``3 * factorial\textless3 - 1\textgreater::value''.  The compiler
 does the subtraction during compile-time,
 so ``factorial\textless3\textgreater::value'' expands to
 ``3 * factorial\textless2\textgreater::value''.
 This recursion terminates on ``factorial\textless0\textgreater::value''
 on line 7\footnote{Even though
 the base case of ``factorial\textless0\textgreater'' is lexically specified
 after the more general
 case of ``factorial\textless n\textgreater'', templates expand the most
 specific case first.  So the compiler will terminate.}.

   \item
 On line 17, a run-time call to ``fact'', defined on line 10, is declared.
 \end{itemize}

 \subsection{Disassembling the Object File}
 The drastic difference in the generated code can be observed by using ``objdump -D''.

 \begin{code}
 400850: be 06 00 00 00   mov    $0x6,%esi
 400855: bf c0 0d 60 00   mov    $0x600dc0,%edi
 40085a: e8 41 fe ff ff   callq  4006a0 <_ZNSolsEi@plt>
 .......
 .......
 .......
 40086c: bf 03 00 00 00   mov    $0x3,%edi
 400871: e8 a0 ff ff ff   callq  400816 <_Z4facti>
 400876: 89 c6            mov    %eax,%esi
 400878: bf c0 0d 60 00   mov    $0x600dc0,%edi
 40087d: e8 1e fe ff ff   callq  4006a0 <_ZNSolsEi@plt>
 \end{code}

 \begin{itemize}
   \item
 The instructions at memory locations 400850 through 40085a correspond to the
 printing of the compile-time expanded call to factorial\textless3\textgreater::value.
 The immediate value 6 is loaded into the ``esi'' register; then the following
 two lines call the printing routine\footnote{at least I assume, because
 I don't completely understand how C++ name-mangling works}.
   \item
 The instructions at locations 40086c through 40087d correspond to the
 printing of the run-time calculation to ``fact(3)''.  The immediate value 3
 is loaded into the ``edi'' register, fact is invoked, the result of
 calling fact is moved from the ``eax'' register to the ``esi'' register, and then
 printing routine is called.

 \end{itemize}
 The compile-time computation worked as expected!

 \section{libbug}
 Like C++'s compile-time language, libbug's is Turing complete.  But libbug's compile-time
 language is the exact same language as the run-time language!

 \begin{code}
 {at-both-times
  {define fact
    [|n| (if (= n 0)
             [1]
             [(* n (fact (- n 1)))])]}}

 (pp {at-compile-time-expand (fact 3)})
 (pp (fact 3))
 \end{code}

 \begin{itemize}
   \item
      On line 1, the ``at-both-times'' macro is invoked, taking the unevaluated
      definition of ``fact'' as
      as argument, interpreting it at compile-time, and compiling it for use at runtime.
   \item
      On lines 2-5, the definition of the ``fact''.
   \item
      On line 7, ``at-compile-time-expand'' is a macro which takes unevaluated code,
      evaluates it to a new form which is then compiled by the compiler.  At compile-time the code
      will expand to ``(pp 6)''.
   \item
      On line 8, the run-time calculation of ``(fact 3)''.
 \end{itemize}

 \subsection{Inspecting the Gambit VM Bytecode}
 By compiling the Scheme source to the ``gvm'' intermediate
 representation, the previously stated behavior can be verified.

 \begin{code}
  r1 = '6
  r0 = #4
  jump/safe fs=4 global[pp] nargs=1
#4 fs=4 return-point
  r1 = '3
  r0 = #5
  jump/safe fs=4 global[fact] nargs=1
#5 fs=4 return-point
  r0 = frame[1]
  jump/poll fs=4 #6
#6 fs=4
  jump/safe fs=0 global[pp] nargs=1
 \end{code}

 \begin{itemize}
   \item
      Lines 1-4 correspond to ``(pp {at-compile-time-expand (fact 3)})''.  The precomputed
      value of ``(fact 3)'' is 6, which is directly stored into a GVM register, and
      then the ``pp'' routine is called to print it.
   \item
      Lines 5-12 correspond to ``(pp (fact 3))''.  3 is stored in a GVM register, ``fact''
      is called, the result of which is passed to ``pp''.
 \end{itemize}

 \section{Comparison of Power}

 Although the compile-time languages both of C++ and of libbug are Turing complete,
 they vary in actual real-world programming power.  The language used
 for compile-time calculation of ``fact'' in C++ is a drastically different language than
 the one used for run-time.  Although not fully demonstrated in this book,
 C++ template metaprogramming relies exclusively on recursion for repetition (it has no
 looping construct), it has no mutable state, and it lacks the ability to do input/output
 (I/O)\footnote{For the masochist who wants to know more about C++'s compile-time language,
 I recommend \cite{ctm} }

 In contrast, the compile-time
 language in libbug is the exact same language as the one that the compiler
 is compiling, complete with state and I/O!  How can that power be used?
 This book is the beginning of an answer.

\chapter{Acknowledgments}

Thanks to Dr. Marc Feeley, for Gambit Scheme, for his mailing list postings
which inspired the foundations of this book, and for reviewing this
book.  Thanks to Adam from the Gambit mailing lists for reviewing the book,
as well as his suggestion for naming convention standards.

Thanks to Dr. John McCarthy for Lisp.

Thanks to Dr. Gerald Sussman and Dr. Guy Steele Jr for Scheme.

Thanks to Dr. Paul Graham for ``On Lisp'', not only for the excellent macros,
but also for demonstrating why writing well matters.

Thanks to Dr. Donald Knuth for \TeX, and thanks to all contributors to
\LaTeX.

Thanks to Dr. Alan Kay for Smalltalk, the first language I loved.  Lisp may be the best high-level language, but Smalltalk is the best high-level environment.

And most importantly, thanks to my wife Teresa, for everything.

\chapter{Related Work}
\begin{itemize}
        \item  Jonathan Blow. https://www.youtube.com/watch?v=UTqZNujQOlA
        \item  ``Compile-time Unit Testing'',
           Aron Barath and Zoltan Porkolab, Eotvos Lorand University, \newline
           http://ceur-ws.org/Vol-1375/SQAMIA2015\_Paper1.pdf
\end{itemize}

 \bibliography{abbr_long,pubext}
\begin{thebibliography}{9}

\bibitem[Abelson96]{sicp}
  Abelon, Harold, Gerald Jay Sussman, and Julie Sussman.
  \emph{Structure and Interpretation of Computer Programs},
  The MIT Press, Massachusetts,
  Second Edition,
  1996.
\bibitem[Abrahams2004]{ctm}
  Abrahams, David and Aleksey Gurtovoy
  \emph{C++ Template Metaprogramming},
  Addison Wesley
  2004.

\bibitem[Church51]{calculi}
  Church, Alonzo
  \emph{The Calculi of Lambda-Conversion},
  Princeton University Press, New Jersey,
  Second Printing,
  1951.

\bibitem[Dybvig03]{schemeprogramminglanguage}
  Dybvig, R. Kent.
  \emph{The Scheme Programming Language},
  The MIT Press, Massachusetts,
  Third Edition,
  2003.

\bibitem[Feeley12]{evalduringmacroexpansion}
  Feeley, Marc. https://mercure.iro.umontreal.ca/pipermail/gambit-list/2012-April/005917.html, 2012

\bibitem[Friedman96]{littleschemer}
  Friedman, Daniel P., and Matthias Felleisen
  \emph{The Scheme Programming Language},
  The MIT Press, Massachusetts,
  Fourth Edition,
  1996.
\bibitem[Graham94]{onlisp}
  Graham, Paul.
  \emph{On Lisp},
  Prentice Hall, New Jersey,
  1994.

\bibitem[Graham96]{ansicl}
  Graham, Paul.
  \emph{ANSI Common Lisp},
  Prentice Hall, New Jersey,
  1996.

\bibitem[Harvey01]{ss}
  Harvey, Brian and Matthew Wright.
  \emph{Simply Scheme - Introducing Computer Science},
  The MIT Press, Massachusetts,
  Second Edition,
  2001.

\bibitem[Hopcroft01]{hmu2001}
  Hopcroft, John E., Rajeev Motwani, and Jeffrey D. Ullman.
  \emph{Introduction to Automata Theory, Languages, and Computation},
  Addison Wesley, Massachusetts,
  Second Edition,
  2001.

\bibitem[Kiselyov98]{setf}
  Kiselyov, Oleg. http://okmij.org/ftp/Scheme/setf.txt , 1998.
\bibitem[Knuth97]{taocp}
  Knuth, Donald E.
  \emph{The Art Of Computer Programming, Volume 1},
  Addison Wesley, Massachusetts,
  Third Edition,
  1997.
\bibitem[Norvig92]{paip}
  Norvig, Peter
  \emph{Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp},
  San Francisco, CA
  1992.
\bibitem[Pierce02]{tapl}
  Pierce, Benjamin C.
  \emph{Types and Programming Languages},
  The MIT Press
  Cambridge, Massachusetts
  2002.
\bibitem[Stallings03]{crypto}
  Stallings, William
  \emph{Cryptography and Network Security},
  Pearson Education, Upper Saddle River, New Jersey,
  Third Edition,
  2002.
\bibitem[Steele90]{cl}
  Steele Jr, Guy L.
  \emph{Common Lisp the Language},
  Digital Press,
  1990.




\end{thebibliography}

\cleardoublepage

 \printindex

\end{document}  %End of document.
