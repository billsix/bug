//Copyright 2014-2018 - William Emerison Six
//All rights reserved
//Distributed under LGPL 2.1 or Apache 2.0

[appendix]
== Compile-Time Language
[[appendix1]]
This appendixfootnote:[Examples in the appendix will have boxes
and line numbers around the code, but they are not part of libbug.]
provides a quick tour of computer language which is interpreted
by the compiler but which is absent in the generated machine
code.  Examples are provided in
well-known languages to illustrate that
most compilers are also interpreters for a subset of the language.  This
appendix provides a baseline demonstration of compile-time computation
so that the reader may contrast these languages' capabilities with libbug's.
But first, let's discuss what is meant by the words "language","compiler", and
"interpreter".

In "Introduction to Automata Theory, Languages, and Computation", Hopcroft,
Motwani, and Ullman define language as "A set of strings all of which are chosen
from some &#931;^*^, where &#931; is a particular alphabet, is called
a language" <<<hmu2001>>>.

Plainly, that means that an "alphabet" is a set of characters (for instance, ASCII), and
that a computer "language" is defined as all of the possible sequences of characters
from that alphabet which are able to be compiled successfully.

An "interpreter" is a computer program which takes an instance of a specific
computer language as input,
and immediately executes the instructions.  A "compiler" is a computer program
which also takes an instance of a specific computer language as input,
but rather than immediately executing the input language, instead the compiler
translates the input language
into another computer language (typically machine code), which is then output to a file
for interpretationfootnote:[the Central Processing Unit (CPU) can be viewed as an
interpreter which takes machine code as its input] at a later time.

In practice though, the distinction is not binary.  Most compilers do not exclusively
translate from an input language
to an output language; instead, they also interpret a subset of the input
language as part of the compilation process.  So what
types of computations can be performed by this subset of language, and how do
they vary in expressive power?

=== C
Consider the following C code:

[source,C,linenums]
----
#include <stdio.h>
#define square(x) ((x) * (x))
int fact(unsigned int n);
int main(int argc, char* argv[]){
#ifdef DEBUG
  printf("Debug - argc = %d\n", argc);
#endif
 printf("%d\n",square(fact(argc)));
  return 0;
}
int fact(unsigned int n){
  return n == 0
    ? 1
    : n * fact(n-1);
}
----

- On line 1, the #include preprocessor command
is language to be interpreted by the compiler,
instructing the compiler to
read the file "stdio.h"
from the filesystem and to splice the content
into the current C file.  The #include command
itself has no representation in the generated machine code, although the contents
of the included file may.

- Line 2 defines a C macro. A C macro is a procedure definition which
is to be interpreted by the compiler instead of being translated
into the output language.
A C macro takes a text
string as input and transforms it into a new text string as output.
This expansion happens before the compiler does anything
else.  For example, using GCC as a compiler, if you run the C preprocessor
"cpp" on the above C code, you'll see that

[source,C,linenums]
----
  printf("%d\n",square(fact(argc)));
----

expands into

[source,C,linenums]
----
  printf("%d\n",((fact(argc)) * (fact(argc))));
----

before compilation.


-Line 3 defines a procedure prototype so that
the compiler knows the argument types and the return type for a procedure not
yet defined called "fact".
It is language interpreted by the compiler to determine the types for the procedure
call to "fact" on line 8.

-Lines 4 through 10 are a procedure definition which will be
translated into instructions in the generated machine code.  Line 5 however, is language
to be interpreted by the compiler, referencing a variable which is defined
only during compile-time, to detemine whether or not line 6 should be
compiled.

=== C&#43;&#43;

C&#43;&#43; inherits C's macros, but with the additional introduction
of templates, its compile-time language
incidentally became Turing complete;  meaning that
anything that can be
calculated by a computer can be calculated using template expansion
at compile-time.  That's great!  So how does a programmer use this new
expressive power?

The following is an example of calculating the factorial of
3; using C&#43;&#43; procedures for run-time calulation, and C&#43;&#43;'s templates for compile-time
calculation.

[source,cpp,linenums]
----
 #include <iostream>
 template <unsigned int n>
 struct factorial {
     enum { value = n * factorial<n - 1>::value };
 };
 template <>
 struct factorial<0> {
     enum { value = 1 };
 };
 int fact(unsigned int n){
   return n == 0
     ? 1
     : n * fact(n-1);
 }
 int main(int argc, char* argv[]){
   std::cout << factorial<3>::value << std::endl;
   std::cout << fact(3) << std::endl;
   return 0;
 }
----

-Lines 10-14 are the run-time calculation of "fact", identical
to the previous version in C.


-Lines 2-9 are the
template code for the compile-time calculation of "factorial".  Notice
that the language constructs used are drastically different than the
run-time constructs.


-On line 16, "factorial<3>::value" is
language to be interpreted
by the compiler via template expansions.  Template expansions
conditionally match patterns based on types (or values in the case
of integers).  For iteration, templates expand recursively instead of using loops.
In this case,  "factorial<3>::value" expands to
"3 * factorial<3 - 1>::value".  The compiler
does the subtraction during compile-time,
so "factorial<3>::value" expands to
"3 * factorial<2>::value".
This recursion terminates on "factorial<0>::value"
on line 7footnote:[Even though
the base case of "factorial<0>" is lexically specified
after the more general
case of "factorial< n>", templates expand the most
specific case first.  So the compiler will terminate.].

-On line 17, a run-time call to "fact", defined on line 10, is declared.

==== Disassembling the Object File
The drastic difference in the generated code can be observed by using "objdump -D".

[source,txt,linenums]
----
 400850: be 06 00 00 00   mov    $0x6,%esi
 400855: bf c0 0d 60 00   mov    $0x600dc0,%edi
 40085a: e8 41 fe ff ff   callq  4006a0 <_ZNSolsEi@plt>
 .......
 .......
 .......
 40086c: bf 03 00 00 00   mov    $0x3,%edi
 400871: e8 a0 ff ff ff   callq  400816 <_Z4facti>
 400876: 89 c6            mov    %eax,%esi
 400878: bf c0 0d 60 00   mov    $0x600dc0,%edi
 40087d: e8 1e fe ff ff   callq  4006a0 <_ZNSolsEi@plt>
----

- The instructions at memory locations 400850 through 40085a correspond to the
printing of the compile-time expanded call to factorial<3>::value.
The immediate value 6 is loaded into the "esi" register; then the following
two lines call the printing routinefootnote:[at least I assume, because
I don't completely understand how C&#43;&#43; name-mangling works].


- The instructions at locations 40086c through 40087d correspond to the
printing of the run-time calculation to "fact(3)".  The immediate value 3
is loaded into the "edi" register, fact is invoked, the result of
calling fact is moved from the "eax" register to the "esi" register, and then
printing routine is called.

The compile-time computation worked as expected!

=== libbug
Like C&#43;&#43;'s compile-time language, libbug's is Turing complete.  But libbug's compile-time
language is the exact same language as the run-time language!

[source,Scheme,linenums]
----
 {at-both-times
  {define fact
    [|n| (if (= n 0)
             [1]
             [(* n (fact (- n 1)))])]}}

 (pp {at-compile-time-expand (fact 3)})
 (pp (fact 3))
----

- On line 1, the "at-both-times" macro is invoked, taking the unevaluated
definition of "fact" as
as argument, interpreting it at compile-time, and compiling it for use at runtime.


- On lines 2-5, the definition of the "fact".

- On line 7, "at-compile-time-expand" is a macro which takes unevaluated code,
evaluates it to a new form which is then compiled by the compiler.  At compile-time the code
will expand to "(pp 6)".

- On line 8, the run-time calculation of "(fact 3)".

==== Inspecting the Gambit VM Bytecode
By compiling the Scheme source to the "gvm" intermediate
representation, the previously stated behavior can be verified.

[source,txt,linenums]
----
  r1 = '6
  r0 = #4
  jump/safe fs=4 global[pp] nargs=1
#4 fs=4 return-point
  r1 = '3
  r0 = #5
  jump/safe fs=4 global[fact] nargs=1
#5 fs=4 return-point
  r0 = frame[1]
  jump/poll fs=4 #6
#6 fs=4
  jump/safe fs=0 global[pp] nargs=1
----

- Lines 1-4 correspond to "(pp {at-compile-time-expand (fact 3)})".  The precomputed
value of "(fact 3)" is 6, which is directly stored into a GVM register, and
then the "pp" routine is called to print it.

- Lines 5-12 correspond to "(pp (fact 3))".  3 is stored in a GVM register, "fact"
is called, the result of which is passed to "pp".


=== Comparison of Power
Although the compile-time languages both of C&#43;&#43; and of libbug are Turing complete,
they vary in actual real-world programming power.  The language used
for compile-time calculation of "fact" in C&#43;&#43; is a drastically different language than
the one used for run-time.  Although not fully demonstrated in this book,
C&#43;&#43; template metaprogramming relies exclusively on recursion for repetition (it has no
looping construct), it has no mutable state, and it lacks the ability to do input/output
(I/O) footnote:[For the masochist who wants to know more about the C&#43;&#43;'s compile-time language, I recommend <<<ctm>>>]

In contrast, the compile-time
language in libbug is the exact same language as the one that the compiler
is compiling, complete with state and I/O!  How can that power be used?
This book is the beginning of an answer.

[appendix]
== Acknowledgments

Thanks to Dr. Marc Feeley, for Gambit Scheme, for his mailing list postings
which inspired the foundations of this book, and for reviewing this
book.  Thanks to Adam from the Gambit mailing lists for reviewing the book,
as well as his suggestion for naming convention standards.

Thanks to Dr. John McCarthy for Lisp.

Thanks to Dr. Gerald Sussman and Dr. Guy Steele Jr for Scheme.

Thanks to Dr. Paul Graham for "On Lisp", not only for the excellent macros,
but also for demonstrating why writing well matters.

Thanks to Dr. Donald Knuth for TeX, and thanks to all contributors to
LaTeX.

Thanks to Dr. Alan Kay for Smalltalk, the first language I loved.  Lisp may be the best high-level language, but Smalltalk is the best high-level environment.

And most importantly, thanks to my wife Teresa, for everything.

[appendix]
== Related Work

- Jonathan Blow. https://www.youtube.com/watch?v=UTqZNujQOlA
- "Compile-time Unit Testing",
Aron Barath and Zoltan Porkolab, Eotvos Lorand University,
http://ceur-ws.org/Vol-1375/SQAMIA2015\_Paper1.pdf



[bibliography]
Bibliography
------------

[bibliography]
- [[[sicp]]] Abelon, Harold, Gerald Jay Sussman, and Julie Sussman.
  'Structure and Interpretation of Computer Programs',
  The MIT Press, Massachusetts,
  Second Edition,
  1996.

- [[[ctm]]]
  Abrahams, David and Aleksey Gurtovoy
  'C&#43;&#43; Template Metaprogramming',
  Addison Wesley
  2004.

- [[[calculi]]]
  Church, Alonzo
  'The Calculi of Lambda-Conversion',
  Princeton University Press, New Jersey,
  Second Printing,
  1951.

- [[[schemeprogramminglanguage]]]
  Dybvig, R. Kent.
  'The Scheme Programming Language',
  The MIT Press, Massachusetts,
  Third Edition,
  2003.

- [[[evalduringmacroexpansion]]]
  Feeley, Marc. https://mercure.iro.umontreal.ca/pipermail/gambit-list/2012-April/005917.html, 2012

- [[[littleschemer]]]
  Friedman, Daniel P., and Matthias Felleisen
  'The Scheme Programming Language',
  The MIT Press, Massachusetts,
  Fourth Edition,
  1996.
- [[[onlisp]]]
  Graham, Paul.
  'On Lisp',
  Prentice Hall, New Jersey,
  1994.

- [[[ansicl]]]
  Graham, Paul.
  'ANSI Common Lisp',
  Prentice Hall, New Jersey,
  1996.

- [[[ss]]]
  Harvey, Brian and Matthew Wright.
  'Simply Scheme - Introducing Computer Science',
  The MIT Press, Massachusetts,
  Second Edition,
  2001.

- [[[hmu2001]]]
  Hopcroft, John E., Rajeev Motwani, and Jeffrey D. Ullman.
  'Introduction to Automata Theory, Languages, and Computation',
  Addison Wesley, Massachusetts,
  Second Edition,
  2001.

- [[[setf]]]
  Kiselyov, Oleg. http://okmij.org/ftp/Scheme/setf.txt , 1998.

- [[[taocp]]]
  Knuth, Donald E.
  'The Art Of Computer Programming, Volume 1',
  Addison Wesley, Massachusetts,
  Third Edition,
  1997.

- [[[paip]]]
  Norvig, Peter
  'Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp',
  San Francisco, CA
  1992.

- [[[tapl]]]
  Pierce, Benjamin C.
  'Types and Programming Languages',
  The MIT Press
  Cambridge, Massachusetts
  2002.

- [[[crypto]]]
  Stallings, William
  'Cryptography and Network Security',
  Pearson Education, Upper Saddle River, New Jersey,
  Third Edition,
  2002.

- [[[cl]]]
  Steele Jr, Guy L.
  'Common Lisp the Language',
  Digital Press,
  1990.

[index]
== Example Index
